<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
<pre>
review_id	product_id	user_id	rating	review_text
1	P1001	u1	5	Great phone, battery lasts long and camera is good
2	P1001	u2	4	Battery good but camera low light is average
3	P2002	u3	1	Terrible product, stopped working after a week
4	P2002	u4	2	Not worth it. Poor build
5	P3003	u5	5	Excellent. Love the design and battery
6	P1001	u6	3	Okay phone. Camera ok, battery meh
7	P3003	u7	4	Good product, solid performance
8	P4004	u8	5	Amazing quality and value

#!/usr/bin/env python3
import sys, csv

def cleanse(s):
    return s.strip()

reader = csv.DictReader(sys.stdin)
for row in reader:
    try:
        product = cleanse(row.get("product_id",""))
        rating = row.get("rating","").strip()
        if product and rating:
            print(f"{product}\t{rating}")
    except Exception:
        continue


#!/usr/bin/env python3
import sys

current_product = None
sum_ratings = 0.0
count = 0

def output_avg(prod, s, c):
    if prod is None: return
    avg = s / c
    print(f"{prod}\t{avg:.3f}")

for line in sys.stdin:
    line = line.strip()
    if not line: continue
    parts = line.split("\t", 1)
    if len(parts) != 2: continue
    product, rating_s = parts
    try:
        rating = float(rating_s)
    except:
        continue
    if current_product is None:
        current_product = product
        sum_ratings = rating
        count = 1
    elif product == current_product:
        sum_ratings += rating
        count += 1
    else:
        output_avg(current_product, sum_ratings, count)
        current_product = product
        sum_ratings = rating
        count = 1

output_avg(current_product, sum_ratings, count)

#!/usr/bin/env python3
import sys, csv, re

stopwords = set()
try:
    with open('stopwords.txt') as f:
        for w in f:
            w = w.strip().lower()
            if w:
                stopwords.add(w)
except Exception:
    stopwords = set(["the","and","is","it","this","that","a","an","in","on","of","for","to","was","are","but","be","with","i","we","you","they","he","she","as","so","its","my","have","has"])

token_re = re.compile(r"[a-zA-Z0-9']{2,}")

reader = csv.DictReader(sys.stdin)
for row in reader:
    try:
        text = row.get("review_text","")
        text = text.lower()
        for token in token_re.findall(text):
            token = token.strip("'")
            if not token: continue
            if token in stopwords: continue
            print(f"{token}\t1")
    except Exception:
        continue

#!/usr/bin/env python3
import sys

current_word = None
count = 0

def out(word, c):
    if word is None: return
    print(f"{word}\t{c}")

for line in sys.stdin:
    line = line.strip()
    if not line: continue
    parts = line.split("\t", 1)
    if len(parts) != 2: continue
    word, c_s = parts
    try:
        c = int(c_s)
    except:
        continue
    if current_word is None:
        current_word = word
        count = c
    elif word == current_word:
        count += c
    else:
        out(current_word, count)
        current_word = word
        count = c

out(current_word, count)

#!/bin/bash
set -e
# Update HSTREAM_JAR below if your hadoop-streaming jar is in a different location
HSTREAM_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar
USERDIR=/user/$(whoami)
INP=${USERDIR}/reviews_input/reviews.csv

hdfs dfs -rm -r -f ${USERDIR}/reviews_output_avg || true
hdfs dfs -rm -r -f ${USERDIR}/reviews_output_kw || true

echo "Running average rating job..."
hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python3 avg_mapper.py" \
 -reducer "python3 avg_reducer.py" \
 -input ${INP} \
 -output ${USERDIR}/reviews_output_avg

echo "Running keyword count job..."
hadoop jar $HSTREAM_JAR \
 -files kw_mapper.py,kw_reducer.py,stopwords.txt \
 -mapper "python3 kw_mapper.py" \
 -reducer "python3 kw_reducer.py" \
 -input ${INP} \
 -output ${USERDIR}/reviews_output_kw

echo "Copy keyword output locally and sort"
hdfs dfs -cat ${USERDIR}/reviews_output_kw/part-00000 > kw_counts.txt
sort -k2 -nr kw_counts.txt | head -n 20 > top_keywords.txt
echo "Top keywords:"
cat top_keywords.txt


the
and
is
it
this
that
a
an
in
on
of
for
to
was
are
but
be
with
i
we
you
they
he
she
as
so
its
my
have
has
not
by
from


hdfs dfs -mkdir -p /user/$(whoami)/reviews_input
hdfs dfs -put -f reviews.csv /user/$(whoami)/reviews_input/
hdfs dfs -ls -h /user/$(whoami)/reviews_input

find / -name "hadoop-streaming*.jar" 2>/dev/null | head -n 5

HSTREAM_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar  # replace with the path you found

hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python3 avg_mapper.py" \
 -reducer "python3 avg_reducer.py" \
 -input /user/$(whoami)/reviews_input/reviews.csv \
 -output /user/$(whoami)/reviews_output_avg \
 -cmdenv LC_ALL=en_US.UTF-8

 yarn jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python3 avg_mapper.py" \
 -reducer "python3 avg_reducer.py" \
 -input /user/$(whoami)/reviews_input/reviews.csv \
 -output /user/$(whoami)/reviews_output_avg

 hdfs dfs -ls /user/$(whoami)/reviews_output_avg
hdfs dfs -cat /user/$(whoami)/reviews_output_avg/part-00000 | head -n 50

P1001   4.000
P2002   1.500
P3003   4.500
P4004   5.000


hadoop jar $HSTREAM_JAR \
 -files kw_mapper.py,kw_reducer.py,stopwords.txt \
 -mapper "python3 kw_mapper.py" \
 -reducer "python3 kw_reducer.py" \
 -input /user/$(whoami)/reviews_input/reviews.csv \
 -output /user/$(whoami)/reviews_output_kw

 hdfs dfs -ls /user/$(whoami)/reviews_output_kw
hdfs dfs -cat /user/$(whoami)/reviews_output_kw/part-00000 | head -n 50


hdfs dfs -cat /user/$(whoami)/reviews_output_kw/part-00000 > kw_counts.txt
# sort numerically by 2nd column (count) descending
sort -k2 -nr kw_counts.txt | head -n 20 > top_keywords.txt
cat top_keywords.txt



#!/bin/bash
set -e
HSTREAM_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar   # change if needed
USERDIR=/user/$(whoami)
INP=${USERDIR}/reviews_input/reviews.csv

# cleanup outputs if exist
hdfs dfs -rm -r -f ${USERDIR}/reviews_output_avg || true
hdfs dfs -rm -r -f ${USERDIR}/reviews_output_kw || true

echo "Running average rating job..."
hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python3 avg_mapper.py" \
 -reducer "python3 avg_reducer.py" \
 -input ${INP} \
 -output ${USERDIR}/reviews_output_avg

echo "Running keyword count job..."
hadoop jar $HSTREAM_JAR \
 -files kw_mapper.py,kw_reducer.py,stopwords.txt \
 -mapper "python3 kw_mapper.py" \
 -reducer "python3 kw_reducer.py" \
 -input ${INP} \
 -output ${USERDIR}/reviews_output_kw

echo "Copy keyword output locally and sort"
hdfs dfs -cat ${USERDIR}/reviews_output_kw/part-00000 > kw_counts.txt
sort -k2 -nr kw_counts.txt | head -n 20 > top_keywords.txt
echo "Top keywords:"
cat top_keywords.txt

#!/bin/bash
set -e
HSTREAM_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar   # change if needed
USERDIR=/user/$(whoami)
INP=${USERDIR}/reviews_input/reviews.csv

# cleanup outputs if exist
hdfs dfs -rm -r -f ${USERDIR}/reviews_output_avg || true
hdfs dfs -rm -r -f ${USERDIR}/reviews_output_kw || true

echo "Running average rating job..."
hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python3 avg_mapper.py" \
 -reducer "python3 avg_reducer.py" \
 -input ${INP} \
 -output ${USERDIR}/reviews_output_avg

echo "Running keyword count job..."
hadoop jar $HSTREAM_JAR \
 -files kw_mapper.py,kw_reducer.py,stopwords.txt \
 -mapper "python3 kw_mapper.py" \
 -reducer "python3 kw_reducer.py" \
 -input ${INP} \
 -output ${USERDIR}/reviews_output_kw

echo "Copy keyword output locally and sort"
hdfs dfs -cat ${USERDIR}/reviews_output_kw/part-00000 > kw_counts.txt
sort -k2 -nr kw_counts.txt | head -n 20 > top_keywords.txt
echo "Top keywords:"
cat top_keywords.txt



./run_all.sh



new



sudo mkdir -p /etc/yum.repos.d/backup
sudo mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup/




sudo tee /etc/yum.repos.d/CentOS-Vault.repo > /dev/null <<'EOF'
[base]
name=CentOS-$releasever - Base
baseurl=http://vault.centos.org/6.10/os/$basearch/
gpgcheck=0

[updates]
name=CentOS-$releasever - Updates
baseurl=http://vault.centos.org/6.10/updates/$basearch/
gpgcheck=0

[extras]
name=CentOS-$releasever - Extras
baseurl=http://vault.centos.org/6.10/extras/$basearch/
gpgcheck=0
EOF


sudo yum clean all
sudo yum makecache


sudo yum install -y python36 python36-devel
python3 --version




new now

sudo sed -i '/^gpgcheck=0/a sslverify=0' /etc/yum.repos.d/CentOS-Vault.repo

sudo yum clean all && sudo yum makecache

date

curl -I http://vault.centos.org/6.10/os/x86_64/repodata/repomd.xml
curl -I https://vault.centos.org/6.10/os/x86_64/repodata/repomd.xml

python - <<'PY'
import urllib2
print 'HTTP ok' if urllib2.urlopen('http://vault.centos.org/6.10/os/x86_64/repodata/repomd.xml').getcode()==200 else 'HTTP fail'
PY






Disha

product_id	,attribute	,score
Product103	,Appearance	,2
Product180	Price	2
Product093	Price	1
Product015	Appearance	3
Product107	Durability	1
Product072	Price	2
Product189	Features	2
Product021	Quality	4
Product103	Performance	4
Product122	Durability	3
Product075	Design	4
Product088	Value	4
Product117	Durability	3
Product100	Durability	5
Product104	Durability	5





#!/usr/bin/env python
# avg_mapper.py  (Python 2.6.6 compatible)
import sys
import csv

reader = csv.reader(sys.stdin)
for row in reader:
    if not row:
        continue
    # Expect at least 3 columns: product, attribute, score
    # Adjust indices if your file places them differently
    try:
        product = row[0].strip()
        attribute = row[1].strip()
        score = row[2].strip()
    except IndexError:
        # skip malformed rows
        continue

    # skip header if present
    if product.lower() == 'product_id' or attribute.lower() == 'attribute' or score.lower() == 'score':
        continue

    # validate numeric score
    try:
        float(score)
    except:
        continue

    # combine product and attribute into single key using '|' as internal delimiter
    key = product + '|' + attribute
    # Hadoop streaming expects "key \t value"
    print "%s\t%s" % (key, score)


    chmod +x avg_mapper.py




    #!/usr/bin/env python
# avg_reducer.py  (Python 2.6.6 compatible)
import sys

current_key = None
sum_scores = 0.0
count = 0

def emit(key, sum_s, cnt):
    # split key back to product and attribute
    parts = key.split('|', 1)
    if len(parts) == 2:
        product, attribute = parts
    else:
        product = key
        attribute = ""
    avg = (sum_s / cnt) if cnt else 0.0
    # print: product \t attribute \t average \t count
    print "%s\t%s\t%.4f\t%d" % (product, attribute, avg, cnt)

for line in sys.stdin:
    line = line.strip()
    if not line:
        continue
    parts = line.split('\t', 1)
    if len(parts) != 2:
        continue
    key, score_str = parts
    try:
        score = float(score_str)
    except:
        continue

    if current_key == key:
        sum_scores += score
        count += 1
    else:
        if current_key:
            emit(current_key, sum_scores, count)
        current_key = key
        sum_scores = score
        count = 1

# final key
if current_key:
    emit(current_key, sum_scores, count)


chmod +x avg_reducer.py



# run mapper, sort (grouping step), run reducer
cat sample.csv | python avg_mapper.py | sort -k1,1 | python avg_reducer.py
ProductA    Design    4.5000    2
ProductA    Price     3.0000    1
ProductB    Design    2.0000    1
ProductB    Price     4.0000    1




# create hdfs input dir
hadoop fs -mkdir -p /user/$(whoami)/survey_input

# copy local CSV to HDFS
hadoop fs -put -f survey.csv /user/$(whoami)/survey_input/


/usr/lib/hadoop-mapreduce/hadoop-streaming.jar

find / -name "hadoop-streaming*.jar" 2>/dev/null

HSTREAM_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar


hadoop fs -rm -r -skipTrash /user/$(whoami)/survey_output_avg || true


hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python avg_mapper.py" \
 -reducer "python avg_reducer.py" \
 -input /user/$(whoami)/survey_input/* \
 -output /user/$(whoami)/survey_output_avg


 hadoop fs -ls /user/$(whoami)/survey_output_avg
hadoop fs -cat /user/$(whoami)/survey_output_avg/part-00000 | less


output ProductA<TAB>Design<TAB>4.5000<TAB>2
...





jar error 




ls -l /usr/lib/hadoop-mapreduce/hadoop-streaming.jar || echo "NOT FOUND: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar"


# search common install directories
sudo find /usr /opt /var -name "hadoop-streaming*.jar" 2>/dev/null

# if Cloudera/HDInsight/HDP style install:
sudo find /usr/hdp /opt/cloudera /usr/lib -name "hadoop-streaming*.jar" 2>/dev/null


find /usr /opt -name "hadoop-streaming*.jar" 2>/dev/null



hadoop classpath | tr ':' '\n' | grep -i streaming || true

hadoop version


# list installed rpm packages that look like hadoop
rpm -qa | grep -i hadoop

# list files from packages (may be noisy)
rpm -qa | grep -i hadoop | xargs -r rpm -ql 2>/dev/null | grep -i streaming

# updatedb may require sudo
sudo updatedb 2>/dev/null && locate hadoop-streaming | head -20

/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar


export HSTREAM_JAR=/usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar
echo $HSTREAM_JAR
# then run:
hadoop jar $HSTREAM_JAR -files avg_mapper.py,avg_reducer.py -mapper "python avg_mapper.py" -reducer "python avg_reducer.py" -input /user/$(whoami)/survey_input/* -output /user/$(whoami)/survey_output_avg



# 1. check direct path
ls -l /usr/lib/hadoop-mapreduce/hadoop-streaming.jar || echo "path not present"

# 2. quick search common dirs (sudo preferred)
sudo find /usr /opt /var -name "hadoop-streaming*.jar" 2>/dev/null

# 3. inspect hadoop classpath (if hadoop is on PATH)
hadoop classpath | tr ':' '\n' | grep -i streaming || true

# 4. if found, export and run job
# export HSTREAM_JAR=/path/you/found && hadoop jar $HSTREAM_JAR <your-args>



hadoop version
hadoop classpath | tr ':' '\n' | grep -i streaming || true




final

# create input directory (safe to re-run)
hadoop fs -mkdir -p /user/cloudera/survey_input

# put dataset into HDFS
hadoop fs -put -f /mnt/data/survey_big.csv /user/cloudera/survey_input/

# verify dataset is in HDFS
hadoop fs -ls /user/cloudera/survey_input

# run the streaming job
hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "python avg_mapper.py" \
 -reducer "python avg_reducer.py" \
 -input /user/cloudera/survey_input/* \
 -output /user/cloudera/survey_output_avg




 hadoop fs -ls /user/cloudera/survey_output_avg
hadoop fs -cat /user/cloudera/survey_output_avg/part-* | head -n 50





hadoop fs -mkdir -p /user/cloudera/survey_input
hadoop fs -put -f /home/cloudera/survey.csv /user/cloudera/survey_input/
hadoop fs -ls /user/cloudera/survey_input


hadoop fs -rm -r -skipTrash /user/cloudera/survey_output_avg || true


export HSTREAM_JAR=/usr/lib/hadoop-mapreduce/hadoop-streaming.jar   # or /usr/hdp/... if that's what you found
ls -l $HSTREAM_JAR
which python2 || which python2.6 || python --version



hadoop jar $HSTREAM_JAR \
 -files avg_mapper.py,avg_reducer.py \
 -mapper "/usr/bin/python2.6 avg_mapper.py" \
 -reducer "/usr/bin/python2.6 avg_reducer.py" \
 -input /user/cloudera/survey_input/* \
 -output /user/cloudera/survey_output_avg


 hadoop fs -ls /user/cloudera/survey_output_avg
hadoop fs -cat /user/cloudera/survey_output_avg/part-* | head -n 50




Quick troubleshooting checklist

If the Hadoop job fails, fetch YARN logs for the application id shown in the job output:

yarn logs -applicationId <appId> 2>/dev/null | less

    


</pre>
</body>
</html>